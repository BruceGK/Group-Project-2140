{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2fbec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mig/Desktop/pitt/projects/Group-Project-2140/cord_ir\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d578cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.elastic_index_reader import IndexReader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from search.data_loader import DataLoader\n",
    "import pandas as pd\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f808acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "queryTree = ET.parse('../data/2020-07-16/eval/topics-rnd5.xml')\n",
    "queryRoot = queryTree.getroot()\n",
    "queries = []\n",
    "for child in queryRoot:\n",
    "    query = {\n",
    "        'queryNo': child.attrib['number'],\n",
    "        'query': child.find('query').text,\n",
    "        'question': child.find('question').text,\n",
    "        'narrative': child.find('narrative').text\n",
    "    }\n",
    "    queries.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6dd60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"../data/models\").mkdir(parents=True, exist_ok=True)\n",
    "loader = DataLoader('../data/2020-07-16')\n",
    "loader.load_metadata_mappings(loader.load_metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ffd731",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = IndexReader()\n",
    "# iterator for the training documents\n",
    "class DocIter:\n",
    "    def __init__(self, pbar=True):\n",
    "        metadata = loader.load_metadata()\n",
    "        # only use rows that have file info\n",
    "        self.metadata = metadata[pd.notna(metadata['pmc_json_files']) | pd.notna(metadata['pdf_json_files'])]\n",
    "        self.rows = self.metadata.shape[0]\n",
    "        self.current = 0\n",
    "        if pbar:\n",
    "            self.pbar = tqdm(total=self.rows)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.rows\n",
    "\n",
    "    def __next__(self): \n",
    "        if self.current < self.rows:\n",
    "            row = self.metadata.iloc[self.current]\n",
    "            self.current += 1\n",
    "            docData = loader.load_paper_data(row)\n",
    "            text = docData['data']['main_text']\n",
    "#             tokens = [t['token'] for t in reader.tokenize(text)['tokens']]\n",
    "#             text = ' '.join(tokens)\n",
    "            if hasattr(self, 'pbar'):\n",
    "                self.pbar.update(1)\n",
    "            return text\n",
    "        if hasattr(self, 'pbar'):\n",
    "            self.pbar.close()\n",
    "        raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d618be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "docIterator = DocIter()\n",
    "vectorizer.fit(docIterator)\n",
    "dump(vectorizer, '../data/models/tfidf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4da55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load('../data/models/tfidf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d41b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1643295"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f0f3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f9e18da12146648eb1820dc82f97ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/info-retri/lib/python3.9/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "judgments = {q['queryNo']: [] for q in queries}\n",
    "with open('../data/2020-07-16/eval/qrels-covid_d5_j0.5-5.txt', 'r') as qrels:\n",
    "    for line in qrels:\n",
    "        [topicId, iteration, cordId, judgment] = line.strip('\\n').split(' ')\n",
    "        judgments[topicId].append({\n",
    "            'iteration': iteration,\n",
    "            'cordId': cordId,\n",
    "            'judgment': max(0, int(judgment))\n",
    "        })\n",
    "# candidate set, select  non-relevant docs in results to add in training data\n",
    "def getRetrievalResults(queries, field):\n",
    "    results = {}\n",
    "    for query in tqdm(queries):\n",
    "        res = reader.search(\"cord_test\", query[field], size=1000, fields=[], highlight=False)\n",
    "        results[query['queryNo']] = res['hits']['hits']\n",
    "    return results\n",
    "candidates = getRetrievalResults(queries, 'question')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37dc107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba41534d6ee4999b7657ce59f326b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate the training data\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "# feature X for each row is [[tfidf of document], [tfidf of query]]\n",
    "# separate some queries to used in evaluation\n",
    "def get_training_data(testQueries=5):\n",
    "    X_train = coo_matrix((0, len(vocabulary) * 2))\n",
    "    y_train = []\n",
    "    X_test_query = []\n",
    "    X_test_question = []\n",
    "    y_test = []\n",
    "    group_counts = []\n",
    "    origin_data_train = {}\n",
    "    origin_data_test = {}\n",
    "    for (i, query) in tqdm(enumerate(queries), total=len(queries)):\n",
    "        queryNo = query['queryNo']\n",
    "        queryTfIdf = vectorizer.transform([query['query']])\n",
    "        questionTfIdf = vectorizer.transform([query['question']])\n",
    "        narrativeTfIdf = vectorizer.transform([query['narrative']])\n",
    "        retrieved = candidates[queryNo]\n",
    "        queryResults = list(judgments[queryNo])\n",
    "        for doc in retrieved:\n",
    "            docId = doc['_id']\n",
    "            if len([e for e in queryResults if e['cordId'] == docId]) == 0:\n",
    "                queryResults.append({\n",
    "                    'cordId': docId,\n",
    "                    'judgment': 0\n",
    "                })\n",
    "        text_list = []\n",
    "        empty_data_index = set()\n",
    "        for (k, item) in enumerate(queryResults):\n",
    "            paper_data = loader.load_paper_data(item['cordId'])\n",
    "            if paper_data:\n",
    "                main_text = paper_data['data']['main_text']\n",
    "                text_list.append(main_text)\n",
    "            else:\n",
    "                empty_data_index.add(k)\n",
    "        queryResults = [queryResults[k] for k in range(len(queryResults)) if k not in empty_data_index]\n",
    "        # batch transform\n",
    "        textTfIdf = vectorizer.transform(text_list)\n",
    "        queryTfIdf = vstack([queryTfIdf for i in range(len(queryResults))])\n",
    "        questionTfIdf = vstack([questionTfIdf for i in range(len(queryResults))])\n",
    "        narrativeTfIdf = vstack([narrativeTfIdf for i in range(len(queryResults))])\n",
    "        queryRows = hstack([textTfIdf, queryTfIdf])\n",
    "        questionRows = hstack([textTfIdf, questionTfIdf])\n",
    "        narrativeRows = hstack([textTfIdf, narrativeTfIdf])\n",
    "        if i < len(queries) - testQueries:\n",
    "            y_train.extend(map(lambda e: e['judgment'], queryResults))\n",
    "            y_train.extend(map(lambda e: e['judgment'], queryResults))\n",
    "            y_train.extend(map(lambda e: e['judgment'], queryResults))\n",
    "            X_train = vstack([X_train, queryRows, questionRows, narrativeRows])\n",
    "            group_counts.append(len(queryResults) * 3)\n",
    "            origin_data_train[queryNo] = queryResults\n",
    "        else:\n",
    "            y_test.append(list(map(lambda e: e['judgment'], queryResults)))\n",
    "            query_vecs = coo_matrix((0, len(vocabulary) * 2))\n",
    "            question_vecs = coo_matrix((0, len(vocabulary) * 2))\n",
    "            query_vecs = vstack([query_vecs, queryRows])\n",
    "            question_vecs = vstack([question_vecs, questionRows])\n",
    "            X_test_query.append(query_vecs)\n",
    "            X_test_question.append(question_vecs)\n",
    "            origin_data_test[queryNo] = queryResults\n",
    "    return (X_train, y_train, X_test_query, X_test_question, y_test, group_counts, origin_data_train, origin_data_test)\n",
    "(X_train, y_train, X_test_query, X_test_question, y_test, group_counts, origin_data_train, origin_data_test) = get_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e7a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/origin_data_test.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(X_train, '../data/models/X_train.joblib')\n",
    "dump(y_train, '../data/models/y_train.joblib')\n",
    "dump(X_test_query, '../data/models/X_test_query.joblib')\n",
    "dump(X_test_question, '../data/models/X_test_question.joblib')\n",
    "dump(y_test, '../data/models/y_test.joblib')\n",
    "dump(group_counts, '../data/models/group_counts.joblib')\n",
    "dump(origin_data_train, '../data/models/origin_data_train.joblib')\n",
    "dump(origin_data_test, '../data/models/origin_data_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ca0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "def getRanker():\n",
    "    ranker = lgb.LGBMRanker(\n",
    "        num_leaves=63,\n",
    "        n_estimators=1000,\n",
    "        max_bin=511,\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        ndcg_eval_at=[1, 3, 5, 10, 15, 20, 25, 30, 50],\n",
    "        learning_rate= .1,\n",
    "        num_iterations=120,\n",
    "        importance_type=\"gain\")\n",
    "    return ranker\n",
    "# ranker = xgb.XGBRanker(objective='rank:ndcg',\n",
    "#       learning_rate=0.1,\n",
    "#       gamma=1.0,\n",
    "#       min_child_weight=0.1,\n",
    "#       max_depth=6,\n",
    "#       verbose=2,\n",
    "#       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28205c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/info-retri/lib/python3.9/site-packages/lightgbm/sklearn.py:621: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/info-retri/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "ranker = getRanker()\n",
    "ranker = ranker.fit(X_train, y_train, group=group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a35b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_eval_at = [3, 5, 10, 15, 20, 25, 30]\n",
    "recall_eval_at = [5, 10, 20, 50, 100, 300]\n",
    "def getEvaluation(results, judgments):\n",
    "    queryEval = []\n",
    "    for queryNo in results:\n",
    "        # doc to relevant score\n",
    "        relMap = {}\n",
    "        # number of docs in each relevant score\n",
    "        relCount = {}\n",
    "        totalRel = 0\n",
    "        for j in judgments[queryNo]:\n",
    "            score = int(j['judgment'])\n",
    "            relMap[j['cordId']] = score\n",
    "            relCount[score] = relCount.get(score, 0) + 1\n",
    "            if score > 0:\n",
    "                totalRel += 1\n",
    "        truePositive = 0\n",
    "        falsePositive = 0\n",
    "        precisions = []\n",
    "        reciprocalRank = 0\n",
    "        cumulativeGain = 0\n",
    "        discountedCumulativeGain = 0\n",
    "        idealDCG = 0\n",
    "        dcgAt = {}\n",
    "        idcgAt = {}\n",
    "        ndcgAt = {}\n",
    "        index = 1\n",
    "        recallAt = {}\n",
    "        for score in sorted(relCount.keys(), reverse=True):\n",
    "            for i in range(relCount[score]):\n",
    "                idealDCG += (2 ** score - 1) / (math.log2(1 + index))\n",
    "                if index in ndcg_eval_at:\n",
    "                    idcgAt[index] = idealDCG\n",
    "                index += 1\n",
    "        for (index, doc) in enumerate(results[queryNo]):\n",
    "            docId = doc['cordId']\n",
    "            cumulativeGain += relMap.get(docId, 0)\n",
    "            discountedCumulativeGain += (2 ** (relMap.get(docId, 0)) - 1) / (math.log2(1 + (1 + i)))\n",
    "            if relMap.get(docId, 0) > 0:\n",
    "                truePositive += 1\n",
    "                # recall increase\n",
    "                precisions.append(truePositive / (truePositive + falsePositive))\n",
    "                if reciprocalRank == 0:\n",
    "                    reciprocalRank = 1 / (index+1)\n",
    "            else:\n",
    "                falsePositive += 1\n",
    "            if index + 1 in ndcg_eval_at and (index + 1) in idcgAt:\n",
    "                dcgAt[index + 1] = discountedCumulativeGain\n",
    "                ndcgAt[index + 1] = dcgAt[index + 1] / idcgAt[index + 1]\n",
    "            if index + 1 in recall_eval_at:\n",
    "                recallAt[index + 1] = truePositive / totalRel\n",
    "        queryEval.append({\n",
    "            'AveragePrecision': sum(precisions) / totalRel,\n",
    "            'ReciprocalRank': reciprocalRank,\n",
    "            'CG': cumulativeGain,\n",
    "            'DCG': discountedCumulativeGain,\n",
    "            'IDCG': idealDCG,\n",
    "            'nDCG': discountedCumulativeGain / idealDCG,\n",
    "            'nDCGAt': ndcgAt,\n",
    "            'recallAt': recallAt\n",
    "        })\n",
    "    return {\n",
    "        'MeanAveragePrecisions': sum(map(lambda e: e['AveragePrecision'], queryEval)) / len(queryEval),\n",
    "        'MeanReciprocalRank': sum(map(lambda e: e['ReciprocalRank'], queryEval)) / len(queryEval),\n",
    "        'AverageNDCG': sum(map(lambda e: e['nDCG'], queryEval)) / len(queryEval),\n",
    "        'AverageNDCGAt': {k: sum(map(lambda e: e['nDCGAt'][k], queryEval)) / len(queryEval) for k in ndcg_eval_at},\n",
    "        'AverageRecallAt': {k: sum(map(lambda e: e['recallAt'][k], queryEval)) / len(queryEval) for k in recall_eval_at}\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0239fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/info-retri/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "query_results = {}\n",
    "question_results = {}\n",
    "for i in range(len(X_test_query)):\n",
    "    queryNo = queries[len(queries) - 5 + i]['queryNo']\n",
    "    origin = origin_data_test[queryNo]\n",
    "    query_result_vec = ranker.predict(X_test_query[i])\n",
    "    question_result_vec = ranker.predict(X_test_question[i])\n",
    "    query_result_indexes = sorted(range(query_result_vec.shape[0]), key=lambda k: query_result_vec[k], reverse=True)\n",
    "    question_result_indexes = sorted(range(question_result_vec.shape[0]), key=lambda k: question_result_vec[k], reverse=True)\n",
    "    query_result = [{'cordId': origin[index]['cordId']} for index in query_result_indexes]\n",
    "    question_result = [{'cordId': origin[index]['cordId']} for index in question_result_indexes]\n",
    "    query_results[queryNo] = query_result\n",
    "    question_results[queryNo] = question_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bfef162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MeanAveragePrecisions': 0.28020915255982576,\n",
       " 'MeanReciprocalRank': 0.53,\n",
       " 'AverageNDCG': 0.487209377322056,\n",
       " 'AverageNDCGAt': {3: 0.04257206767999946,\n",
       "  5: 0.03938906903656014,\n",
       "  10: 0.05658203396797654,\n",
       "  15: 0.06826037269364724,\n",
       "  20: 0.07352287735734579,\n",
       "  25: 0.07788491722148343,\n",
       "  30: 0.08333441807055461},\n",
       " 'AverageRecallAt': {5: 0.00789045568569007,\n",
       "  10: 0.016599118700500376,\n",
       "  20: 0.0346930761177999,\n",
       "  50: 0.08694460780535793,\n",
       "  100: 0.17200189443065533,\n",
       "  300: 0.4270305777269218}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEvaluation(query_results, origin_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5e80a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MeanAveragePrecisions': 0.21130033721096364,\n",
       " 'MeanReciprocalRank': 0.1788936627282492,\n",
       " 'AverageNDCG': 0.487209377322056,\n",
       " 'AverageNDCGAt': {3: 0.017892875669912107,\n",
       "  5: 0.02186918185859056,\n",
       "  10: 0.022651808116935755,\n",
       "  15: 0.03439818944247965,\n",
       "  20: 0.043447986463499136,\n",
       "  25: 0.048028482736797835,\n",
       "  30: 0.04834396735609978},\n",
       " 'AverageRecallAt': {5: 0.004396637865161935,\n",
       "  10: 0.00879327573032387,\n",
       "  20: 0.02249007439217239,\n",
       "  50: 0.058733777558250086,\n",
       "  100: 0.10488699706699658,\n",
       "  300: 0.35708902937820924}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEvaluation(question_results, origin_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57076f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb649e2d74cf40f38cf5916e64bedb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full data to train\n",
    "(X_train, y_train, X_test_query, X_test_question, y_test, group_counts, origin_data_train, origin_data_test) = get_training_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "137f897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = getRanker()\n",
    "ranker = ranker.fit(X_train, y_train, group=group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a53b665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/ranker.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(ranker, '../data/models/ranker.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a9905ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c652ba891041fe804f241e8e657ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docIterator = DocIter()\n",
    "docMatrix = vectorizer.transform(docIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0442b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/docMatrix.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = loader.load_metadata()\n",
    "dump({\n",
    "    'matrix': docMatrix,\n",
    "    'cordIds': list(metadata[pd.notna(metadata['pmc_json_files']) | pd.notna(metadata['pdf_json_files'])]['cord_uid'])\n",
    "}, '../data/models/docMatrix.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb2694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78245e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33d12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c4d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e5d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905913e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
